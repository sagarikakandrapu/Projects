{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "751c7fb8-50d8-442a-857d-b2b289d199c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "import sqlite3\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from typing import Iterable, List, Dict, Any, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    base_dir: str = \".\"\n",
    "    src_db: str = \"b2b_source.sqlite\"\n",
    "    dwh_db: str = \"b2b_dwh.sqlite\"\n",
    "    marketing_xlsx: str = \"marketing_leads.xlsx\"\n",
    "    weblog_file: str = \"weblogs_combined.log\"\n",
    "\n",
    "paths = Paths()\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "N_COMPANIES = 220          # includes suppliers and buyers\n",
    "N_SUPPLIERS = 55\n",
    "N_PRODUCTS = 650\n",
    "N_END_CUSTOMERS = 60_000\n",
    "N_ORDERS = 140_000\n",
    "AVG_ITEMS_PER_ORDER = 2.4  # will randomize\n",
    "MAX_ITEMS_PER_ORDER = 6\n",
    "\n",
    "LOOKBACK_DAYS = 520  # generate > last year so reports make sense\n",
    "\n",
    "INCR_NEW_ORDERS = 8_000\n",
    "INCR_NEW_LOG_LINES = 25_000\n",
    "\n",
    "COUNTRIES = [\"AR\", \"BR\", \"CL\", \"CO\", \"MX\", \"US\", \"ES\"]\n",
    "COUNTRY_WEIGHTS = [0.36, 0.12, 0.08, 0.07, 0.12, 0.15, 0.10]\n",
    "\n",
    "DEVICE_UA_POOL = [\n",
    "    # Desktop\n",
    "    (\"desktop\", \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"),\n",
    "    (\"desktop\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.3 Safari/605.1.15\"),\n",
    "    (\"desktop\", \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"),\n",
    "    # Mobile\n",
    "    (\"mobile\", \"Mozilla/5.0 (iPhone; CPU iPhone OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1\"),\n",
    "    (\"mobile\", \"Mozilla/5.0 (Linux; Android 13; Pixel 7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36\"),\n",
    "    # Tablet\n",
    "    (\"tablet\", \"Mozilla/5.0 (iPad; CPU OS 16_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1\"),\n",
    "    # Bots\n",
    "    (\"bot\", \"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ef0a04-9bae-4ce8-9774-fe8ca70f595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_print(msg: str) -> None:\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts}] {msg}\")\n",
    "\n",
    "def utc_now() -> datetime:\n",
    "    return datetime.now(timezone.utc)\n",
    "\n",
    "def dt_to_iso(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.astimezone(timezone.utc).isoformat()\n",
    "\n",
    "def iso_to_dt(s: str) -> datetime:\n",
    "    # SQLite stores ISO strings; parse back\n",
    "    return datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "def chunks(seq: List[Any], size: int) -> Iterable[List[Any]]:\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i:i + size]\n",
    "\n",
    "def rand_str(n: int) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
    "\n",
    "def weighted_choice(items: List[Any], weights: List[float]) -> Any:\n",
    "    return random.choices(items, weights=weights, k=1)[0]\n",
    "\n",
    "def connect(db_path: str) -> sqlite3.Connection:\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    conn.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "    conn.execute(\"PRAGMA foreign_keys=ON;\")\n",
    "    return conn\n",
    "\n",
    "def exec_script(conn: sqlite3.Connection, sql: str) -> None:\n",
    "    conn.executescript(sql)\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a037b4-c3e2-4eb4-a546-19c53d4624f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created source DB: b2b_source.sqlite\n"
     ]
    }
   ],
   "source": [
    "SOURCE_SCHEMA_SQL = \"\"\"\n",
    "DROP TABLE IF EXISTS companies;\n",
    "DROP TABLE IF EXISTS suppliers;\n",
    "DROP TABLE IF EXISTS end_customers;\n",
    "DROP TABLE IF EXISTS products;\n",
    "DROP TABLE IF EXISTS supplier_products;\n",
    "DROP TABLE IF EXISTS company_catalog;\n",
    "DROP TABLE IF EXISTS orders;\n",
    "DROP TABLE IF EXISTS order_items;\n",
    "DROP TABLE IF EXISTS ip_geo_map;\n",
    "DROP TABLE IF EXISTS etl_source_audit;\n",
    "\n",
    "CREATE TABLE companies (\n",
    "    company_id      INTEGER PRIMARY KEY,\n",
    "    cuit            TEXT NOT NULL UNIQUE,\n",
    "    name            TEXT NOT NULL,\n",
    "    country_code    TEXT NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE suppliers (\n",
    "    supplier_id     INTEGER PRIMARY KEY,\n",
    "    company_id      INTEGER NOT NULL UNIQUE,\n",
    "    qualification_tier TEXT NOT NULL,\n",
    "    FOREIGN KEY(company_id) REFERENCES companies(company_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE end_customers (\n",
    "    end_customer_id INTEGER PRIMARY KEY,\n",
    "    document_number TEXT NOT NULL UNIQUE,\n",
    "    full_name       TEXT NOT NULL,\n",
    "    date_of_birth   TEXT NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE products (\n",
    "    product_id      INTEGER PRIMARY KEY,\n",
    "    sku             TEXT NOT NULL UNIQUE,\n",
    "    name            TEXT NOT NULL,\n",
    "    category        TEXT NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE supplier_products (\n",
    "    supplier_product_id INTEGER PRIMARY KEY,\n",
    "    supplier_id     INTEGER NOT NULL,\n",
    "    product_id      INTEGER NOT NULL,\n",
    "    default_price   REAL NOT NULL,\n",
    "    active_flag     INTEGER NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL,\n",
    "    UNIQUE(supplier_id, product_id),\n",
    "    FOREIGN KEY(supplier_id) REFERENCES suppliers(supplier_id),\n",
    "    FOREIGN KEY(product_id) REFERENCES products(product_id)\n",
    ");\n",
    "\n",
    "-- each buyer company defines its own catalog price for products coming from suppliers\n",
    "CREATE TABLE company_catalog (\n",
    "    catalog_id      INTEGER PRIMARY KEY,\n",
    "    buyer_company_id INTEGER NOT NULL,\n",
    "    product_id      INTEGER NOT NULL,\n",
    "    supplier_id     INTEGER NOT NULL,\n",
    "    catalog_price   REAL NOT NULL,\n",
    "    effective_from_utc TEXT NOT NULL,\n",
    "    effective_to_utc   TEXT,\n",
    "    active_flag     INTEGER NOT NULL,\n",
    "    FOREIGN KEY(buyer_company_id) REFERENCES companies(company_id),\n",
    "    FOREIGN KEY(product_id) REFERENCES products(product_id),\n",
    "    FOREIGN KEY(supplier_id) REFERENCES suppliers(supplier_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE orders (\n",
    "    order_id        INTEGER PRIMARY KEY,\n",
    "    buyer_company_id INTEGER NOT NULL,\n",
    "    end_customer_id INTEGER NOT NULL,\n",
    "    order_ts_utc    TEXT NOT NULL,\n",
    "    status          TEXT NOT NULL,\n",
    "    currency        TEXT NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL,\n",
    "    FOREIGN KEY(buyer_company_id) REFERENCES companies(company_id),\n",
    "    FOREIGN KEY(end_customer_id) REFERENCES end_customers(end_customer_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "    order_item_id   INTEGER PRIMARY KEY,\n",
    "    order_id        INTEGER NOT NULL,\n",
    "    product_id      INTEGER NOT NULL,\n",
    "    supplier_id     INTEGER NOT NULL,\n",
    "    qty             INTEGER NOT NULL,\n",
    "    unit_price      REAL NOT NULL,\n",
    "    line_total      REAL NOT NULL,\n",
    "    created_at_utc  TEXT NOT NULL,\n",
    "    FOREIGN KEY(order_id) REFERENCES orders(order_id),\n",
    "    FOREIGN KEY(product_id) REFERENCES products(product_id),\n",
    "    FOREIGN KEY(supplier_id) REFERENCES suppliers(supplier_id)\n",
    ");\n",
    "\n",
    "-- for deterministic \"geo\" without external API\n",
    "CREATE TABLE ip_geo_map (\n",
    "    ip              TEXT PRIMARY KEY,\n",
    "    country_code    TEXT NOT NULL,\n",
    "    city            TEXT NOT NULL\n",
    ");\n",
    "\n",
    "-- optional: a tiny audit table in source to mimic “last successful load”\n",
    "CREATE TABLE etl_source_audit (\n",
    "    id              INTEGER PRIMARY KEY,\n",
    "    entity_name     TEXT NOT NULL,\n",
    "    last_change_id  INTEGER,\n",
    "    last_change_ts_utc TEXT,\n",
    "    updated_at_utc  TEXT NOT NULL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "src_conn = connect(paths.src_db)\n",
    "exec_script(src_conn, SOURCE_SCHEMA_SQL)\n",
    "src_conn.close()\n",
    "\n",
    "print(\"Created source DB:\", paths.src_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d081c1ff-7e0f-4541-8695-ea148b303adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded source data.\n",
      "Companies: 220 Suppliers: 55 Products: 650\n",
      "Customers: 60000 Orders: 140000 Order items: 282405\n"
     ]
    }
   ],
   "source": [
    "def build_companies(n_companies: int, n_suppliers: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    now = utc_now()\n",
    "    companies = []\n",
    "    for i in range(1, n_companies + 1):\n",
    "        country = weighted_choice(COUNTRIES, COUNTRY_WEIGHTS)\n",
    "        cuit = f\"{random.randint(20, 33)}-{random.randint(10_000_000, 99_999_999)}-{random.randint(0,9)}\"\n",
    "        name = f\"{random.choice(['Aurum','Delta','Nimbus','Vertex','Orion','Pampa','Andes','Pacifica'])} {random.choice(['Trading','Supply','Wholesale','Industrial','Market'])} {rand_str(3)}\"\n",
    "        created = now - timedelta(days=random.randint(30, LOOKBACK_DAYS))\n",
    "        companies.append((i, cuit, name, country, dt_to_iso(created)))\n",
    "\n",
    "    companies_df = pd.DataFrame(companies, columns=[\"company_id\",\"cuit\",\"name\",\"country_code\",\"created_at_utc\"])\n",
    "\n",
    "    supplier_company_ids = set(random.sample(list(companies_df[\"company_id\"]), k=n_suppliers))\n",
    "    suppliers = []\n",
    "    tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "    for sid, cid in enumerate(sorted(supplier_company_ids), start=1):\n",
    "        suppliers.append((sid, cid, weighted_choice(tiers, [0.25, 0.45, 0.30])))\n",
    "\n",
    "    suppliers_df = pd.DataFrame(suppliers, columns=[\"supplier_id\",\"company_id\",\"qualification_tier\"])\n",
    "    return companies_df, suppliers_df\n",
    "\n",
    "def build_products(n_products: int) -> pd.DataFrame:\n",
    "    now = utc_now()\n",
    "    categories = [\"office\", \"safety\", \"packaging\", \"electronics\", \"tools\", \"cleaning\", \"food-service\", \"lab\"]\n",
    "    items = []\n",
    "    for pid in range(1, n_products + 1):\n",
    "        cat = random.choice(categories)\n",
    "        sku = f\"{cat[:3].upper()}-{random.randint(100000, 999999)}\"\n",
    "        name = f\"{random.choice(['Pro','Ultra','Eco','Prime','Max'])} {random.choice(['Kit','Bundle','Pack','Unit','Case'])} {random.choice(['A','B','C','X','Z'])}-{random.randint(1,99)}\"\n",
    "        created = now - timedelta(days=random.randint(15, LOOKBACK_DAYS))\n",
    "        items.append((pid, sku, name, cat, dt_to_iso(created)))\n",
    "    return pd.DataFrame(items, columns=[\"product_id\",\"sku\",\"name\",\"category\",\"created_at_utc\"])\n",
    "\n",
    "def build_supplier_products(suppliers_df: pd.DataFrame, products_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    now = utc_now()\n",
    "    rows = []\n",
    "    sp_id = 1\n",
    "    for _, s in suppliers_df.iterrows():\n",
    "        # each supplier carries a subset\n",
    "        carry = random.randint(int(N_PRODUCTS * 0.25), int(N_PRODUCTS * 0.55))\n",
    "        product_ids = random.sample(list(products_df[\"product_id\"]), k=carry)\n",
    "        for pid in product_ids:\n",
    "            base = round(random.uniform(3.0, 900.0), 2)\n",
    "            active = 1 if random.random() > 0.03 else 0\n",
    "            created = now - timedelta(days=random.randint(1, LOOKBACK_DAYS))\n",
    "            rows.append((sp_id, int(s[\"supplier_id\"]), int(pid), base, active, dt_to_iso(created)))\n",
    "            sp_id += 1\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"supplier_product_id\",\"supplier_id\",\"product_id\",\"default_price\",\"active_flag\",\"created_at_utc\"\n",
    "    ])\n",
    "\n",
    "def build_company_catalog(companies_df: pd.DataFrame, suppliers_df: pd.DataFrame, supplier_products_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    now = utc_now()\n",
    "    supplier_ids = set(suppliers_df[\"supplier_id\"].tolist())\n",
    "    buyer_company_ids = [cid for cid in companies_df[\"company_id\"].tolist() if cid not in set(suppliers_df[\"company_id\"].tolist())]\n",
    "\n",
    "    rows = []\n",
    "    catalog_id = 1\n",
    "\n",
    "    # make a lookup of active supplier_products by supplier\n",
    "    active_sp = supplier_products_df[supplier_products_df[\"active_flag\"] == 1].copy()\n",
    "    by_supplier = {}\n",
    "    for sid, grp in active_sp.groupby(\"supplier_id\"):\n",
    "        by_supplier[int(sid)] = grp[[\"product_id\",\"default_price\"]].values.tolist()\n",
    "\n",
    "    for buyer_cid in random.sample(buyer_company_ids, k=int(len(buyer_company_ids) * 0.85)):\n",
    "        # each buyer picks several suppliers\n",
    "        chosen_suppliers = random.sample(list(supplier_ids), k=random.randint(8, min(18, len(supplier_ids))))\n",
    "        for sid in chosen_suppliers:\n",
    "            if sid not in by_supplier:\n",
    "                continue\n",
    "            # each supplier contributes a subset to the buyer's catalog\n",
    "            sp_rows = by_supplier[sid]\n",
    "            take = random.randint(40, 120)\n",
    "            for pid, default_price in random.sample(sp_rows, k=min(take, len(sp_rows))):\n",
    "                # negotiate: +/- up to 15%\n",
    "                factor = random.uniform(0.85, 1.15)\n",
    "                catalog_price = round(float(default_price) * factor, 2)\n",
    "                eff_from = now - timedelta(days=random.randint(1, LOOKBACK_DAYS))\n",
    "                rows.append((catalog_id, int(buyer_cid), int(pid), int(sid), catalog_price, dt_to_iso(eff_from), None, 1))\n",
    "                catalog_id += 1\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        \"catalog_id\",\"buyer_company_id\",\"product_id\",\"supplier_id\",\"catalog_price\",\n",
    "        \"effective_from_utc\",\"effective_to_utc\",\"active_flag\"\n",
    "    ])\n",
    "\n",
    "# header - build end_customers (stable constraints, controlled bad DOB only)\n",
    "\n",
    "def build_end_customers(n_customers: int) -> pd.DataFrame:\n",
    "    now = utc_now()\n",
    "    first = [\"Juan\",\"Sofia\",\"Mateo\",\"Valentina\",\"Martin\",\"Camila\",\"Bruno\",\"Lucia\",\"Diego\",\"Ana\",\"Tomas\",\"Renata\"]\n",
    "    last = [\"Gomez\",\"Perez\",\"Lopez\",\"Diaz\",\"Fernandez\",\"Silva\",\"Alvarez\",\"Romero\",\"Torres\",\"Ruiz\",\"Sosa\",\"Vega\"]\n",
    "\n",
    "    rows = []\n",
    "    for i in range(1, n_customers + 1):\n",
    "        dob = datetime(1958, 1, 1, tzinfo=timezone.utc) + timedelta(days=random.randint(0, 60 * 365))\n",
    "        name = f\"{random.choice(first)} {random.choice(last)} {random.choice(last)}\"\n",
    "\n",
    "        # Always unique + NOT NULL for SQLite constraints\n",
    "        doc = f\"DOC-{i:08d}\"\n",
    "\n",
    "        created = now - timedelta(days=random.randint(1, LOOKBACK_DAYS))\n",
    "        rows.append((i, doc, name, dob.date().isoformat(), dt_to_iso(created)))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"end_customer_id\",\"document_number\",\"full_name\",\"date_of_birth\",\"created_at_utc\"])\n",
    "\n",
    "    # Inject a small number of invalid DOB strings (won't break DB constraints)\n",
    "    bad_idx = random.sample(range(len(df)), k=35)\n",
    "    df.loc[bad_idx[:20], \"date_of_birth\"] = \"1900-02-30\"   # invalid date string\n",
    "    df.loc[bad_idx[20:], \"date_of_birth\"] = \"not-a-date\"   # another invalid pattern\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_orders_and_items(\n",
    "    companies_df: pd.DataFrame,\n",
    "    suppliers_df: pd.DataFrame,\n",
    "    catalog_df: pd.DataFrame,\n",
    "    end_customers_df: pd.DataFrame,\n",
    "    n_orders: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    now = utc_now()\n",
    "\n",
    "    supplier_company_ids = set(suppliers_df[\"company_id\"].tolist())\n",
    "    buyer_company_ids = [cid for cid in companies_df[\"company_id\"].tolist() if cid not in supplier_company_ids]\n",
    "\n",
    "    # catalog lookup for buyer -> list of (product_id, supplier_id, price)\n",
    "    cat_active = catalog_df[catalog_df[\"active_flag\"] == 1].copy()\n",
    "    cat_map: Dict[int, List[Tuple[int,int,float]]] = {}\n",
    "    for buyer_id, grp in cat_active.groupby(\"buyer_company_id\"):\n",
    "        cat_map[int(buyer_id)] = [(int(r.product_id), int(r.supplier_id), float(r.catalog_price)) for r in grp.itertuples(index=False)]\n",
    "\n",
    "    statuses = [\"created\", \"confirmed\", \"shipped\", \"delivered\", \"cancelled\"]\n",
    "    status_w = [0.08, 0.22, 0.18, 0.47, 0.05]\n",
    "\n",
    "    orders = []\n",
    "    items = []\n",
    "    order_item_id = 1\n",
    "\n",
    "    customer_ids = end_customers_df[\"end_customer_id\"].tolist()\n",
    "\n",
    "    for order_id in range(1, n_orders + 1):\n",
    "        buyer = random.choice(buyer_company_ids)\n",
    "        if buyer not in cat_map or len(cat_map[buyer]) < 20:\n",
    "            buyer = random.choice([b for b in buyer_company_ids if b in cat_map])\n",
    "\n",
    "        cust_id = random.choice(customer_ids)\n",
    "        order_ts = now - timedelta(days=random.randint(0, LOOKBACK_DAYS), seconds=random.randint(0, 86400))\n",
    "        status = weighted_choice(statuses, status_w)\n",
    "        currency = \"USD\" if random.random() < 0.18 else \"ARS\"\n",
    "\n",
    "        created = order_ts - timedelta(minutes=random.randint(1, 120))\n",
    "        orders.append((order_id, int(buyer), int(cust_id), dt_to_iso(order_ts), status, currency, dt_to_iso(created)))\n",
    "\n",
    "        k = min(MAX_ITEMS_PER_ORDER, max(1, int(random.gauss(mu=AVG_ITEMS_PER_ORDER, sigma=1.1))))\n",
    "        picks = random.sample(cat_map[buyer], k=k)\n",
    "        for (pid, sid, unit_price) in picks:\n",
    "            qty = random.randint(1, 12)\n",
    "            # occasional dirty row: negative qty or zero price\n",
    "            if random.random() < 0.0015:\n",
    "                qty = -qty\n",
    "            if random.random() < 0.0015:\n",
    "                unit_price = 0.0\n",
    "\n",
    "            line_total = round(float(unit_price) * qty, 2)\n",
    "            items.append((order_item_id, order_id, pid, sid, qty, float(unit_price), float(line_total), dt_to_iso(order_ts)))\n",
    "            order_item_id += 1\n",
    "\n",
    "    orders_df = pd.DataFrame(orders, columns=[\n",
    "        \"order_id\",\"buyer_company_id\",\"end_customer_id\",\"order_ts_utc\",\"status\",\"currency\",\"created_at_utc\"\n",
    "    ])\n",
    "    items_df = pd.DataFrame(items, columns=[\n",
    "        \"order_item_id\",\"order_id\",\"product_id\",\"supplier_id\",\"qty\",\"unit_price\",\"line_total\",\"created_at_utc\"\n",
    "    ])\n",
    "    return orders_df, items_df\n",
    "\n",
    "def build_ip_geo_map(companies_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create IPs per company-country; weblog generator will reuse these\n",
    "    cities = {\n",
    "        \"AR\": [\"Buenos Aires\", \"Cordoba\", \"Rosario\"],\n",
    "        \"BR\": [\"Sao Paulo\", \"Rio\", \"Curitiba\"],\n",
    "        \"CL\": [\"Santiago\", \"Valparaiso\"],\n",
    "        \"CO\": [\"Bogota\", \"Medellin\"],\n",
    "        \"MX\": [\"CDMX\", \"Guadalajara\"],\n",
    "        \"US\": [\"Miami\", \"Dallas\", \"NYC\"],\n",
    "        \"ES\": [\"Madrid\", \"Barcelona\"],\n",
    "    }\n",
    "\n",
    "    rows = []\n",
    "    used = set()\n",
    "\n",
    "    def gen_ip() -> str:\n",
    "        # keep it within private-ish looking ranges but still readable\n",
    "        return f\"{random.randint(11, 219)}.{random.randint(0,255)}.{random.randint(0,255)}.{random.randint(1,254)}\"\n",
    "\n",
    "    # one or more IP per company\n",
    "    for c in companies_df.itertuples(index=False):\n",
    "        for _ in range(random.randint(1, 4)):\n",
    "            ip = gen_ip()\n",
    "            while ip in used:\n",
    "                ip = gen_ip()\n",
    "            used.add(ip)\n",
    "            rows.append((ip, c.country_code, random.choice(cities[c.country_code])))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"ip\",\"country_code\",\"city\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_df(conn: sqlite3.Connection, df: pd.DataFrame, table: str, chunksize: int = 5_000) -> None:\n",
    "    # method=None uses executemany under the hood and avoids SQLite parameter limits from multi-row INSERT\n",
    "    df.to_sql(\n",
    "        table,\n",
    "        conn,\n",
    "        if_exists=\"append\",\n",
    "        index=False,\n",
    "        chunksize=chunksize,\n",
    "        method=None\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Build datasets\n",
    "companies_df, suppliers_df = build_companies(N_COMPANIES, N_SUPPLIERS)\n",
    "products_df = build_products(N_PRODUCTS)\n",
    "supplier_products_df = build_supplier_products(suppliers_df, products_df)\n",
    "catalog_df = build_company_catalog(companies_df, suppliers_df, supplier_products_df)\n",
    "end_customers_df = build_end_customers(N_END_CUSTOMERS)\n",
    "orders_df, order_items_df = build_orders_and_items(companies_df, suppliers_df, catalog_df, end_customers_df, N_ORDERS)\n",
    "ip_geo_df = build_ip_geo_map(companies_df)\n",
    "\n",
    "# Load into SQLite\n",
    "src_conn = connect(paths.src_db)\n",
    "src_conn.execute(\"BEGIN;\")\n",
    "load_df(src_conn, companies_df, \"companies\")\n",
    "load_df(src_conn, suppliers_df, \"suppliers\")\n",
    "load_df(src_conn, products_df, \"products\")\n",
    "load_df(src_conn, supplier_products_df, \"supplier_products\")\n",
    "load_df(src_conn, catalog_df, \"company_catalog\")\n",
    "load_df(src_conn, end_customers_df, \"end_customers\")\n",
    "load_df(src_conn, orders_df, \"orders\")\n",
    "load_df(src_conn, order_items_df, \"order_items\")\n",
    "load_df(src_conn, ip_geo_df, \"ip_geo_map\")\n",
    "src_conn.commit()\n",
    "src_conn.close()\n",
    "\n",
    "print(\"Loaded source data.\")\n",
    "print(\"Companies:\", len(companies_df), \"Suppliers:\", len(suppliers_df), \"Products:\", len(products_df))\n",
    "print(\"Customers:\", len(end_customers_df), \"Orders:\", len(orders_df), \"Order items:\", len(order_items_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f86ea85-80b4-4405-8231-802cded99c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created marketing spreadsheet: marketing_leads.xlsx rows: 18000\n"
     ]
    }
   ],
   "source": [
    "def build_marketing_leads(n_rows: int = 18_000) -> pd.DataFrame:\n",
    "    now = utc_now()\n",
    "    lead_sources = [\"linkedin\", \"webinar\", \"partner\", \"cold-email\", \"referral\", \"conference\"]\n",
    "    stages = [\"new\", \"contacted\", \"qualified\", \"won\", \"lost\"]\n",
    "    industries = [\"manufacturing\", \"retail\", \"healthcare\", \"logistics\", \"construction\", \"hospitality\", \"energy\"]\n",
    "\n",
    "    rows = []\n",
    "    for i in range(1, n_rows + 1):\n",
    "        country = weighted_choice(COUNTRIES, COUNTRY_WEIGHTS)\n",
    "        created = now - timedelta(days=random.randint(0, 365), hours=random.randint(0, 23))\n",
    "        company_name = f\"{random.choice(['North','South','East','West','Global','Central'])} {random.choice(['Edge','Bridge','Core','Line','Peak'])} {random.choice(['LLC','SA','Inc','SRL'])} {rand_str(3)}\"\n",
    "        email = f\"lead_{i}_{rand_str(4).lower()}@example.com\"\n",
    "        stage = weighted_choice(stages, [0.46, 0.22, 0.18, 0.06, 0.08])\n",
    "        score = max(0, min(100, int(random.gauss(58, 18))))\n",
    "\n",
    "        rows.append((i, company_name, email, country, random.choice(industries), random.choice(lead_sources), stage, score, dt_to_iso(created)))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\n",
    "        \"lead_id\",\"company_name\",\"email\",\"country_code\",\"industry\",\"source\",\"stage\",\"lead_score\",\"created_at_utc\"\n",
    "    ])\n",
    "\n",
    "    # inject a few bad rows\n",
    "    bad_idx = random.sample(range(len(df)), k=45)\n",
    "    df.loc[bad_idx[:15], \"email\"] = \"not-an-email\"\n",
    "    df.loc[bad_idx[15:30], \"lead_score\"] = 900\n",
    "    df.loc[bad_idx[30:], \"company_name\"] = \"\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "leads_df = build_marketing_leads()\n",
    "with pd.ExcelWriter(paths.marketing_xlsx, engine=\"openpyxl\") as writer:\n",
    "    leads_df.to_excel(writer, index=False, sheet_name=\"leads\")\n",
    "\n",
    "print(\"Created marketing spreadsheet:\", paths.marketing_xlsx, \"rows:\", len(leads_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5219477-ce7b-440f-8d83-7e623c22049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DWH DB: b2b_dwh.sqlite\n"
     ]
    }
   ],
   "source": [
    "DWH_SCHEMA_SQL = \"\"\"\n",
    "DROP TABLE IF EXISTS etl_runs;\n",
    "DROP TABLE IF EXISTS etl_checkpoints;\n",
    "DROP TABLE IF EXISTS etl_row_errors;\n",
    "\n",
    "DROP TABLE IF EXISTS dim_date;\n",
    "DROP TABLE IF EXISTS dim_company;\n",
    "DROP TABLE IF EXISTS dim_end_customer;\n",
    "DROP TABLE IF EXISTS dim_product;\n",
    "DROP TABLE IF EXISTS dim_device;\n",
    "DROP TABLE IF EXISTS dim_marketing_lead;\n",
    "\n",
    "DROP TABLE IF EXISTS fact_sales;\n",
    "DROP TABLE IF EXISTS fact_web_events;\n",
    "\n",
    "CREATE TABLE etl_runs (\n",
    "    run_id          TEXT PRIMARY KEY,\n",
    "    job_name        TEXT NOT NULL,\n",
    "    run_type        TEXT NOT NULL,          -- initial | incremental\n",
    "    status          TEXT NOT NULL,          -- running | succeeded | failed\n",
    "    started_at_utc  TEXT NOT NULL,\n",
    "    finished_at_utc TEXT,\n",
    "    last_step       TEXT,\n",
    "    error_message   TEXT\n",
    ");\n",
    "\n",
    "CREATE TABLE etl_checkpoints (\n",
    "    job_name        TEXT NOT NULL,\n",
    "    entity_name     TEXT NOT NULL,          -- orders | weblogs | leads\n",
    "    last_id         INTEGER,\n",
    "    last_ts_utc     TEXT,\n",
    "    updated_at_utc  TEXT NOT NULL,\n",
    "    PRIMARY KEY(job_name, entity_name)\n",
    ");\n",
    "\n",
    "CREATE TABLE etl_row_errors (\n",
    "    error_id        TEXT PRIMARY KEY,\n",
    "    run_id          TEXT NOT NULL,\n",
    "    source_name     TEXT NOT NULL,          -- b2b_db | weblogs | marketing_xlsx\n",
    "    entity_name     TEXT NOT NULL,\n",
    "    record_ref      TEXT,\n",
    "    error_type      TEXT NOT NULL,\n",
    "    error_message   TEXT NOT NULL,\n",
    "    raw_payload     TEXT,\n",
    "    created_at_utc  TEXT NOT NULL\n",
    ");\n",
    "\n",
    "-- Date dimension for reporting\n",
    "CREATE TABLE dim_date (\n",
    "    date_key        INTEGER PRIMARY KEY,   -- YYYYMMDD\n",
    "    date_iso        TEXT NOT NULL UNIQUE,  -- YYYY-MM-DD\n",
    "    year            INTEGER NOT NULL,\n",
    "    month           INTEGER NOT NULL,\n",
    "    day             INTEGER NOT NULL,\n",
    "    month_start_iso TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_company (\n",
    "    company_key     INTEGER PRIMARY KEY,\n",
    "    company_id      INTEGER NOT NULL UNIQUE,\n",
    "    cuit            TEXT NOT NULL,\n",
    "    name            TEXT NOT NULL,\n",
    "    country_code    TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_end_customer (\n",
    "    end_customer_key INTEGER PRIMARY KEY,\n",
    "    end_customer_id  INTEGER NOT NULL UNIQUE,\n",
    "    document_number  TEXT,\n",
    "    full_name        TEXT,\n",
    "    date_of_birth    TEXT,\n",
    "    is_valid         INTEGER NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_product (\n",
    "    product_key     INTEGER PRIMARY KEY,\n",
    "    product_id      INTEGER NOT NULL UNIQUE,\n",
    "    sku             TEXT NOT NULL,\n",
    "    name            TEXT NOT NULL,\n",
    "    category        TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_device (\n",
    "    device_key      INTEGER PRIMARY KEY,\n",
    "    device_family   TEXT NOT NULL UNIQUE   -- desktop|mobile|tablet|bot|other\n",
    ");\n",
    "\n",
    "CREATE TABLE dim_marketing_lead (\n",
    "    lead_key        INTEGER PRIMARY KEY,\n",
    "    lead_id         INTEGER NOT NULL UNIQUE,\n",
    "    company_name    TEXT,\n",
    "    email           TEXT,\n",
    "    country_code    TEXT,\n",
    "    industry        TEXT,\n",
    "    source          TEXT,\n",
    "    stage           TEXT,\n",
    "    lead_score      INTEGER,\n",
    "    is_valid        INTEGER NOT NULL\n",
    ");\n",
    "\n",
    "-- Facts\n",
    "CREATE TABLE fact_sales (\n",
    "    sales_id        TEXT PRIMARY KEY,\n",
    "    order_id        INTEGER NOT NULL,\n",
    "    order_item_id   INTEGER NOT NULL UNIQUE,\n",
    "    order_ts_utc    TEXT NOT NULL,\n",
    "\n",
    "    date_key        INTEGER NOT NULL,\n",
    "    buyer_company_key INTEGER NOT NULL,\n",
    "    end_customer_key  INTEGER NOT NULL,\n",
    "    product_key     INTEGER NOT NULL,\n",
    "    supplier_company_key INTEGER NOT NULL,\n",
    "\n",
    "    qty             INTEGER NOT NULL,\n",
    "    unit_price      REAL NOT NULL,\n",
    "    line_total      REAL NOT NULL,\n",
    "    currency        TEXT NOT NULL,\n",
    "    order_status    TEXT NOT NULL,\n",
    "\n",
    "    FOREIGN KEY(date_key) REFERENCES dim_date(date_key),\n",
    "    FOREIGN KEY(buyer_company_key) REFERENCES dim_company(company_key),\n",
    "    FOREIGN KEY(end_customer_key) REFERENCES dim_end_customer(end_customer_key),\n",
    "    FOREIGN KEY(product_key) REFERENCES dim_product(product_key),\n",
    "    FOREIGN KEY(supplier_company_key) REFERENCES dim_company(company_key)\n",
    ");\n",
    "\n",
    "CREATE TABLE fact_web_events (\n",
    "    web_event_id    TEXT PRIMARY KEY,\n",
    "    event_ts_utc    TEXT NOT NULL,\n",
    "    date_key        INTEGER NOT NULL,\n",
    "\n",
    "    ip              TEXT NOT NULL,\n",
    "    country_code    TEXT,\n",
    "    username        TEXT,\n",
    "    user_agent      TEXT,\n",
    "    device_key      INTEGER NOT NULL,\n",
    "\n",
    "    http_method     TEXT,\n",
    "    path            TEXT,\n",
    "    status_code     INTEGER,\n",
    "\n",
    "    FOREIGN KEY(date_key) REFERENCES dim_date(date_key),\n",
    "    FOREIGN KEY(device_key) REFERENCES dim_device(device_key)\n",
    ");\n",
    "\n",
    "-- Indexes for report speed\n",
    "CREATE INDEX idx_fact_sales_date ON fact_sales(date_key);\n",
    "CREATE INDEX idx_fact_sales_product ON fact_sales(product_key);\n",
    "CREATE INDEX idx_fact_sales_buyer ON fact_sales(buyer_company_key);\n",
    "\n",
    "CREATE INDEX idx_fact_web_date ON fact_web_events(date_key);\n",
    "CREATE INDEX idx_fact_web_country ON fact_web_events(country_code);\n",
    "CREATE INDEX idx_fact_web_device ON fact_web_events(device_key);\n",
    "\"\"\"\n",
    "\n",
    "dwh_conn = connect(paths.dwh_db)\n",
    "exec_script(dwh_conn, DWH_SCHEMA_SQL)\n",
    "dwh_conn.close()\n",
    "\n",
    "print(\"Created DWH DB:\", paths.dwh_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc509f9a-85e3-4673-b00f-daa726df5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLJob:\n",
    "    def __init__(self, job_name: str, src_db: str, dwh_db: str):\n",
    "        self.job_name = job_name\n",
    "        self.src_db = src_db\n",
    "        self.dwh_db = dwh_db\n",
    "\n",
    "    def _start_run(self, run_type: str) -> str:\n",
    "        run_id = str(uuid.uuid4())\n",
    "        conn = connect(self.dwh_db)\n",
    "        conn.execute(\n",
    "            \"INSERT INTO etl_runs(run_id, job_name, run_type, status, started_at_utc) VALUES (?,?,?,?,?)\",\n",
    "            (run_id, self.job_name, run_type, \"running\", dt_to_iso(utc_now()))\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return run_id\n",
    "\n",
    "    def _set_run_step(self, run_id: str, step: str) -> None:\n",
    "        conn = connect(self.dwh_db)\n",
    "        conn.execute(\"UPDATE etl_runs SET last_step=? WHERE run_id=?\", (step, run_id))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def _finish_run(self, run_id: str, status: str, error_message: Optional[str] = None) -> None:\n",
    "        conn = connect(self.dwh_db)\n",
    "        conn.execute(\n",
    "            \"UPDATE etl_runs SET status=?, finished_at_utc=?, error_message=? WHERE run_id=?\",\n",
    "            (status, dt_to_iso(utc_now()), error_message, run_id)\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def _get_checkpoint(self, entity_name: str) -> Dict[str, Any]:\n",
    "        conn = connect(self.dwh_db)\n",
    "        row = conn.execute(\n",
    "            \"SELECT last_id, last_ts_utc FROM etl_checkpoints WHERE job_name=? AND entity_name=?\",\n",
    "            (self.job_name, entity_name)\n",
    "        ).fetchone()\n",
    "        conn.close()\n",
    "        if not row:\n",
    "            return {\"last_id\": None, \"last_ts_utc\": None}\n",
    "        return {\"last_id\": row[0], \"last_ts_utc\": row[1]}\n",
    "\n",
    "    def _set_checkpoint(self, entity_name: str, last_id: Optional[int], last_ts_utc: Optional[str]) -> None:\n",
    "        conn = connect(self.dwh_db)\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO etl_checkpoints(job_name, entity_name, last_id, last_ts_utc, updated_at_utc)\n",
    "            VALUES (?,?,?,?,?)\n",
    "            ON CONFLICT(job_name, entity_name)\n",
    "            DO UPDATE SET last_id=excluded.last_id, last_ts_utc=excluded.last_ts_utc, updated_at_utc=excluded.updated_at_utc\n",
    "            \"\"\",\n",
    "            (self.job_name, entity_name, last_id, last_ts_utc, dt_to_iso(utc_now()))\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def _log_row_error(\n",
    "        self,\n",
    "        run_id: str,\n",
    "        source_name: str,\n",
    "        entity_name: str,\n",
    "        record_ref: str,\n",
    "        error_type: str,\n",
    "        error_message: str,\n",
    "        raw_payload: Optional[Dict[str, Any]] = None\n",
    "    ) -> None:\n",
    "        conn = connect(self.dwh_db)\n",
    "        conn.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO etl_row_errors(error_id, run_id, source_name, entity_name, record_ref, error_type, error_message, raw_payload, created_at_utc)\n",
    "            VALUES (?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                str(uuid.uuid4()),\n",
    "                run_id,\n",
    "                source_name,\n",
    "                entity_name,\n",
    "                record_ref,\n",
    "                error_type,\n",
    "                error_message[:500],\n",
    "                json.dumps(raw_payload, ensure_ascii=False) if raw_payload is not None else None,\n",
    "                dt_to_iso(utc_now()),\n",
    "            )\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa8a5fa-b409-42a3-a35a-35bc4545d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_PATTERNS = [\n",
    "    (\"bot\", re.compile(r\"bot|spider|crawl\", re.I)),\n",
    "    (\"mobile\", re.compile(r\"iphone|android.+mobile|mobile safari|windows phone\", re.I)),\n",
    "    (\"tablet\", re.compile(r\"ipad|android(?!.*mobile)\", re.I)),\n",
    "    (\"desktop\", re.compile(r\"windows nt|macintosh|x11; linux\", re.I)),\n",
    "]\n",
    "\n",
    "def parse_device_family(user_agent: str) -> str:\n",
    "    if not user_agent:\n",
    "        return \"other\"\n",
    "    for fam, pat in DEVICE_PATTERNS:\n",
    "        if pat.search(user_agent):\n",
    "            return fam\n",
    "    return \"other\"\n",
    "\n",
    "def date_key_from_iso(ts_utc: str) -> int:\n",
    "    d = iso_to_dt(ts_utc).date()\n",
    "    return int(d.strftime(\"%Y%m%d\"))\n",
    "\n",
    "def ensure_dim_date(dwh_conn: sqlite3.Connection, min_date: datetime, max_date: datetime) -> None:\n",
    "    # Build date dimension range once\n",
    "    min_d = min_date.date()\n",
    "    max_d = max_date.date()\n",
    "    rows = []\n",
    "    cur = datetime(min_d.year, min_d.month, min_d.day, tzinfo=timezone.utc)\n",
    "    end = datetime(max_d.year, max_d.month, max_d.day, tzinfo=timezone.utc)\n",
    "\n",
    "    while cur.date() <= end.date():\n",
    "        dk = int(cur.strftime(\"%Y%m%d\"))\n",
    "        date_iso = cur.date().isoformat()\n",
    "        month_start = cur.replace(day=1).date().isoformat()\n",
    "        rows.append((dk, date_iso, cur.year, cur.month, cur.day, month_start))\n",
    "        cur += timedelta(days=1)\n",
    "\n",
    "    dwh_conn.execute(\"BEGIN;\")\n",
    "    dwh_conn.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO dim_date(date_key, date_iso, year, month, day, month_start_iso)\n",
    "        VALUES (?,?,?,?,?,?)\n",
    "        \"\"\",\n",
    "        rows\n",
    "    )\n",
    "    dwh_conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca2c5706-10e1-4861-bb7c-5229962c65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dimensions(run_id: str, src_db: str, dwh_db: str, job_name: str) -> None:\n",
    "    src = connect(src_db)\n",
    "    dwh = connect(dwh_db)\n",
    "\n",
    "    # companies\n",
    "    comp = pd.read_sql_query(\"SELECT company_id, cuit, name, country_code FROM companies\", src)\n",
    "    dwh.execute(\"BEGIN;\")\n",
    "    dwh.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO dim_company(company_id, cuit, name, country_code)\n",
    "        VALUES (?,?,?,?)\n",
    "        \"\"\",\n",
    "        comp[[\"company_id\",\"cuit\",\"name\",\"country_code\"]].itertuples(index=False, name=None)\n",
    "    )\n",
    "    dwh.commit()\n",
    "\n",
    "    # products\n",
    "    prod = pd.read_sql_query(\"SELECT product_id, sku, name, category FROM products\", src)\n",
    "    dwh.execute(\"BEGIN;\")\n",
    "    dwh.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO dim_product(product_id, sku, name, category)\n",
    "        VALUES (?,?,?,?)\n",
    "        \"\"\",\n",
    "        prod[[\"product_id\",\"sku\",\"name\",\"category\"]].itertuples(index=False, name=None)\n",
    "    )\n",
    "    dwh.commit()\n",
    "\n",
    "    # device families (static)\n",
    "    device_fams = [(\"desktop\",), (\"mobile\",), (\"tablet\",), (\"bot\",), (\"other\",)]\n",
    "    dwh.execute(\"BEGIN;\")\n",
    "    dwh.executemany(\"INSERT OR IGNORE INTO dim_device(device_family) VALUES (?)\", device_fams)\n",
    "    dwh.commit()\n",
    "\n",
    "    # customers with validation\n",
    "    cust = pd.read_sql_query(\n",
    "        \"SELECT end_customer_id, document_number, full_name, date_of_birth FROM end_customers\",\n",
    "        src\n",
    "    )\n",
    "\n",
    "    def validate_customer(r: pd.Series) -> Tuple[int, Optional[str]]:\n",
    "        # doc is guaranteed valid in source now\n",
    "        if not r[\"full_name\"] or str(r[\"full_name\"]).strip() == \"\":\n",
    "            return 0, \"missing_full_name\"\n",
    "        try:\n",
    "            datetime.fromisoformat(str(r[\"date_of_birth\"]))\n",
    "        except Exception:\n",
    "            return 0, \"invalid_date_of_birth\"\n",
    "        return 1, None\n",
    "\n",
    "\n",
    "    is_valid = []\n",
    "    for _, r in cust.iterrows():\n",
    "        ok, reason = validate_customer(r)\n",
    "        is_valid.append(ok)\n",
    "        if ok == 0:\n",
    "            # log but still load a row (marked invalid) so facts can still join\n",
    "            ETLJob(job_name, src_db, dwh_db)._log_row_error(\n",
    "                run_id=run_id,\n",
    "                source_name=\"b2b_db\",\n",
    "                entity_name=\"end_customers\",\n",
    "                record_ref=str(r[\"end_customer_id\"]),\n",
    "                error_type=\"validation_error\",\n",
    "                error_message=reason or \"invalid_row\",\n",
    "                raw_payload=r.to_dict(),\n",
    "            )\n",
    "\n",
    "    cust[\"is_valid\"] = is_valid\n",
    "\n",
    "    dwh.execute(\"BEGIN;\")\n",
    "    dwh.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO dim_end_customer(end_customer_id, document_number, full_name, date_of_birth, is_valid)\n",
    "        VALUES (?,?,?,?,?)\n",
    "        \"\"\",\n",
    "        cust[[\"end_customer_id\",\"document_number\",\"full_name\",\"date_of_birth\",\"is_valid\"]].itertuples(index=False, name=None)\n",
    "    )\n",
    "    dwh.commit()\n",
    "\n",
    "    # date dimension range: derive from orders min/max\n",
    "    mm = src.execute(\"SELECT MIN(order_ts_utc), MAX(order_ts_utc) FROM orders\").fetchone()\n",
    "    if mm and mm[0] and mm[1]:\n",
    "        min_dt = iso_to_dt(mm[0])\n",
    "        max_dt = iso_to_dt(mm[1])\n",
    "        ensure_dim_date(dwh, min_dt - timedelta(days=2), max_dt + timedelta(days=2))\n",
    "\n",
    "    src.close()\n",
    "    dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb2d4e5-7293-496b-ae6f-0f904fb72e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_RE = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\n",
    "\n",
    "def load_marketing_leads(run_id: str, dwh_db: str, xlsx_path: str, job: ETLJob) -> None:\n",
    "    dwh = connect(dwh_db)\n",
    "\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=\"leads\")\n",
    "\n",
    "    def validate_lead(r: pd.Series) -> Tuple[int, Optional[str]]:\n",
    "        if not r.get(\"company_name\") or str(r[\"company_name\"]).strip() == \"\":\n",
    "            return 0, \"missing_company_name\"\n",
    "        if not r.get(\"email\") or not EMAIL_RE.match(str(r[\"email\"]).strip()):\n",
    "            return 0, \"invalid_email\"\n",
    "        score = r.get(\"lead_score\")\n",
    "        if score is None or not (0 <= int(score) <= 100):\n",
    "            return 0, \"lead_score_out_of_range\"\n",
    "        return 1, None\n",
    "\n",
    "    is_valid = []\n",
    "    for _, r in df.iterrows():\n",
    "        ok, reason = validate_lead(r)\n",
    "        is_valid.append(ok)\n",
    "        if ok == 0:\n",
    "            job._log_row_error(\n",
    "                run_id=run_id,\n",
    "                source_name=\"marketing_xlsx\",\n",
    "                entity_name=\"leads\",\n",
    "                record_ref=str(r.get(\"lead_id\")),\n",
    "                error_type=\"validation_error\",\n",
    "                error_message=reason or \"invalid_row\",\n",
    "                raw_payload={k: (None if (isinstance(v, float) and math.isnan(v)) else v) for k, v in r.to_dict().items()},\n",
    "            )\n",
    "\n",
    "    df[\"is_valid\"] = is_valid\n",
    "\n",
    "    # Upsert-like: ignore existing, since lead_id is stable for this project\n",
    "    dwh.execute(\"BEGIN;\")\n",
    "    dwh.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO dim_marketing_lead(\n",
    "            lead_id, company_name, email, country_code, industry, source, stage, lead_score, is_valid\n",
    "        ) VALUES (?,?,?,?,?,?,?,?,?)\n",
    "        \"\"\",\n",
    "        df[[\n",
    "            \"lead_id\",\"company_name\",\"email\",\"country_code\",\"industry\",\"source\",\"stage\",\"lead_score\",\"is_valid\"\n",
    "        ]].itertuples(index=False, name=None)\n",
    "    )\n",
    "    dwh.commit()\n",
    "    dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb1447a-513c-404a-91f9-31c3e374e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sales_facts(run_id: str, src_db: str, dwh_db: str, job: ETLJob, run_type: str) -> None:\n",
    "    src = connect(src_db)\n",
    "    dwh = connect(dwh_db)\n",
    "\n",
    "    cp = job._get_checkpoint(\"orders\")\n",
    "    last_order_id = cp[\"last_id\"] if run_type == \"incremental\" else None\n",
    "\n",
    "    if last_order_id is not None:\n",
    "        etl_print(f\"Sales facts incremental load: starting after order_id={last_order_id}\")\n",
    "    else:\n",
    "        etl_print(\"Sales facts initial load: processing all orders\")\n",
    "\n",
    "    where = \"\"\n",
    "    params: Tuple[Any, ...] = ()\n",
    "    if last_order_id is not None:\n",
    "        where = \"WHERE o.order_id > ?\"\n",
    "        params = (int(last_order_id),)\n",
    "\n",
    "    q = f\"\"\"\n",
    "    SELECT\n",
    "        o.order_id, o.buyer_company_id, o.end_customer_id, o.order_ts_utc, o.status, o.currency,\n",
    "        oi.order_item_id, oi.product_id, oi.supplier_id, oi.qty, oi.unit_price, oi.line_total\n",
    "    FROM orders o\n",
    "    JOIN order_items oi ON oi.order_id = o.order_id\n",
    "    {where}\n",
    "    ORDER BY o.order_id, oi.order_item_id\n",
    "    \"\"\"\n",
    "\n",
    "    cur = src.execute(q, params)\n",
    "\n",
    "    max_seen_order_id = last_order_id\n",
    "    batch = []\n",
    "    BATCH_SIZE = 20_000\n",
    "    inserted_ok = 0\n",
    "    skipped_bad = 0\n",
    "\n",
    "    def flush(rows: List[Tuple[Any, ...]]) -> None:\n",
    "        if not rows:\n",
    "            return\n",
    "        dwh.execute(\"BEGIN;\")\n",
    "        dwh.executemany(\n",
    "            \"\"\"\n",
    "            INSERT OR IGNORE INTO fact_sales(\n",
    "                sales_id, order_id, order_item_id, order_ts_utc,\n",
    "                date_key, buyer_company_key, end_customer_key, product_key, supplier_company_key,\n",
    "                qty, unit_price, line_total, currency, order_status\n",
    "            ) VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\",\n",
    "            rows\n",
    "        )\n",
    "        dwh.commit()\n",
    "\n",
    "    etl_print(\"Reading orders + items from source DB\")\n",
    "\n",
    "    while True:\n",
    "        r = cur.fetchone()\n",
    "        if r is None:\n",
    "            break\n",
    "\n",
    "        (\n",
    "            order_id, buyer_company_id, end_customer_id, order_ts_utc, status, currency,\n",
    "            order_item_id, product_id, supplier_id, qty, unit_price, line_total\n",
    "        ) = r\n",
    "\n",
    "        # Validate row\n",
    "        if qty is None or int(qty) <= 0:\n",
    "            skipped_bad += 1\n",
    "            job._log_row_error(\n",
    "                run_id, \"b2b_db\", \"order_items\", str(order_item_id),\n",
    "                \"validation_error\", \"qty_must_be_positive\",\n",
    "                raw_payload={\"order_item_id\": order_item_id, \"order_id\": order_id, \"qty\": qty}\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if unit_price is None or float(unit_price) <= 0:\n",
    "            skipped_bad += 1\n",
    "            job._log_row_error(\n",
    "                run_id, \"b2b_db\", \"order_items\", str(order_item_id),\n",
    "                \"validation_error\", \"unit_price_must_be_positive\",\n",
    "                raw_payload={\"order_item_id\": order_item_id, \"order_id\": order_id, \"unit_price\": unit_price}\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        dk = date_key_from_iso(order_ts_utc)\n",
    "\n",
    "        # supplier_id -> supplier company_id\n",
    "        supplier_company_id = src.execute(\n",
    "            \"SELECT company_id FROM suppliers WHERE supplier_id=?\",\n",
    "            (int(supplier_id),)\n",
    "        ).fetchone()\n",
    "\n",
    "        if not supplier_company_id:\n",
    "            skipped_bad += 1\n",
    "            job._log_row_error(\n",
    "                run_id, \"b2b_db\", \"order_items\", str(order_item_id),\n",
    "                \"lookup_error\", \"supplier_not_found\",\n",
    "                raw_payload={\"supplier_id\": supplier_id}\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        supplier_company_key = int(supplier_company_id[0])\n",
    "\n",
    "        sales_id = str(uuid.uuid4())\n",
    "        batch.append((\n",
    "            sales_id,\n",
    "            int(order_id),\n",
    "            int(order_item_id),\n",
    "            order_ts_utc,\n",
    "            int(dk),\n",
    "            int(buyer_company_id),\n",
    "            int(end_customer_id),\n",
    "            int(product_id),\n",
    "            int(supplier_company_key),\n",
    "            int(qty),\n",
    "            float(unit_price),\n",
    "            float(line_total),\n",
    "            str(currency),\n",
    "            str(status),\n",
    "        ))\n",
    "\n",
    "        max_seen_order_id = int(order_id)\n",
    "\n",
    "        if len(batch) >= BATCH_SIZE:\n",
    "            flush(batch)\n",
    "            inserted_ok += len(batch)\n",
    "            etl_print(f\"Inserted {inserted_ok} sales rows so far | last_order_id={max_seen_order_id} | skipped_bad={skipped_bad}\")\n",
    "            batch = []\n",
    "            job._set_checkpoint(\"orders\", max_seen_order_id, None)\n",
    "\n",
    "    flush(batch)\n",
    "    inserted_ok += len(batch)\n",
    "\n",
    "    if max_seen_order_id is not None:\n",
    "        job._set_checkpoint(\"orders\", int(max_seen_order_id), None)\n",
    "\n",
    "    etl_print(f\"Sales facts load complete | inserted_ok={inserted_ok} | skipped_bad={skipped_bad} | last_order_id={max_seen_order_id}\")\n",
    "\n",
    "    src.close()\n",
    "    dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8697922e-97fe-4ddc-8a16-42e636d7bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "LOG_RE = re.compile(\n",
    "    r'^(?P<ip>\\S+)\\s+\\S+\\s+(?P<user>\\S+)\\s+\\[(?P<ts>[^\\]]+)\\]\\s+'\n",
    "    r'\"(?P<req>[^\"]*)\"\\s+(?P<status>\\d{3})\\s+\\S+\\s+\"[^\"]*\"\\s+\"(?P<ua>[^\"]*)\"'\n",
    ")\n",
    "\n",
    "def parse_apache_ts(ts: str) -> datetime:\n",
    "    # Example: 10/Oct/2000:13:55:36 +0000\n",
    "    return datetime.strptime(ts, \"%d/%b/%Y:%H:%M:%S %z\").astimezone(timezone.utc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffaf63a3-5e72-4061-8172-185f21595614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weblogs(run_id: str, src_db: str, dwh_db: str, log_path: str, job: ETLJob, run_type: str) -> None:\n",
    "    dwh = connect(dwh_db)\n",
    "    src = connect(src_db)\n",
    "\n",
    "    cp = job._get_checkpoint(\"weblogs\")\n",
    "    last_line = int(cp[\"last_id\"] or 0) if run_type == \"incremental\" else 0\n",
    "\n",
    "    etl_print(f\"Weblogs load started | file={log_path}\")\n",
    "    if run_type == \"incremental\":\n",
    "        etl_print(f\"Weblogs incremental resume | last_line={last_line}\")\n",
    "\n",
    "    device_key_map = {r[0]: r[1] for r in dwh.execute(\"SELECT device_family, device_key FROM dim_device\").fetchall()}\n",
    "    ip_geo = {r[0]: r[1] for r in src.execute(\"SELECT ip, country_code FROM ip_geo_map\").fetchall()}\n",
    "\n",
    "    BATCH = 25_000\n",
    "    rows = []\n",
    "    processed_ok = 0\n",
    "    skipped_bad = 0\n",
    "    current_line = 0\n",
    "\n",
    "    def flush() -> None:\n",
    "        nonlocal rows\n",
    "        if not rows:\n",
    "            return\n",
    "        dwh.execute(\"BEGIN;\")\n",
    "        dwh.executemany(\n",
    "            \"\"\"\n",
    "            INSERT OR IGNORE INTO fact_web_events(\n",
    "                web_event_id, event_ts_utc, date_key, ip, country_code, username, user_agent, device_key,\n",
    "                http_method, path, status_code\n",
    "            ) VALUES (?,?,?,?,?,?,?,?,?,?,?)\n",
    "            \"\"\",\n",
    "            rows\n",
    "        )\n",
    "        dwh.commit()\n",
    "        rows = []\n",
    "\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            current_line += 1\n",
    "            if current_line <= last_line:\n",
    "                continue\n",
    "\n",
    "            m = LOG_RE.match(line.strip())\n",
    "            if not m:\n",
    "                skipped_bad += 1\n",
    "                job._log_row_error(\n",
    "                    run_id, \"weblogs\", \"log_lines\", str(current_line),\n",
    "                    \"parse_error\", \"unmatched_log_format\",\n",
    "                    raw_payload={\"line\": line.strip()[:800]}\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            ip = m.group(\"ip\")\n",
    "            user = m.group(\"user\")\n",
    "            ua = m.group(\"ua\") or \"\"\n",
    "            status = int(m.group(\"status\"))\n",
    "            req = (m.group(\"req\") or \"\").split()\n",
    "\n",
    "            try:\n",
    "                ts_utc = parse_apache_ts(m.group(\"ts\"))\n",
    "            except Exception as e:\n",
    "                skipped_bad += 1\n",
    "                job._log_row_error(\n",
    "                    run_id, \"weblogs\", \"log_lines\", str(current_line),\n",
    "                    \"parse_error\", f\"bad_timestamp: {e}\",\n",
    "                    raw_payload={\"ts\": m.group(\"ts\"), \"line\": line.strip()[:800]}\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            method = req[0] if len(req) >= 2 else None\n",
    "            path = req[1] if len(req) >= 2 else None\n",
    "\n",
    "            fam = parse_device_family(ua)\n",
    "            device_key = device_key_map.get(fam, device_key_map[\"other\"])\n",
    "            country = ip_geo.get(ip)\n",
    "            dk = int(ts_utc.strftime(\"%Y%m%d\"))\n",
    "\n",
    "            rows.append((\n",
    "                str(uuid.uuid4()),\n",
    "                dt_to_iso(ts_utc),\n",
    "                dk,\n",
    "                ip,\n",
    "                country,\n",
    "                None if user == \"-\" else user,\n",
    "                ua,\n",
    "                device_key,\n",
    "                method,\n",
    "                path,\n",
    "                status\n",
    "            ))\n",
    "\n",
    "            processed_ok += 1\n",
    "\n",
    "            if processed_ok % BATCH == 0:\n",
    "                flush()\n",
    "                etl_print(f\"Weblogs progress | processed_ok={processed_ok} | skipped_bad={skipped_bad} | current_line={current_line}\")\n",
    "                job._set_checkpoint(\"weblogs\", current_line, None)\n",
    "\n",
    "    flush()\n",
    "    job._set_checkpoint(\"weblogs\", current_line, None)\n",
    "\n",
    "    etl_print(f\"Weblogs load complete | processed_ok={processed_ok} | skipped_bad={skipped_bad} | last_line={current_line}\")\n",
    "\n",
    "    src.close()\n",
    "    dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c7b76a-a228-4903-9a19-a5d01afafd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etl(job: ETLJob, run_type: str = \"initial\") -> str:\n",
    "    etl_print(f\"Starting ETL job '{job.job_name}' | run_type={run_type}\")\n",
    "\n",
    "    run_id = job._start_run(run_type=run_type)\n",
    "    etl_print(f\"Run ID: {run_id}\")\n",
    "\n",
    "    try:\n",
    "        etl_print(\"Step 1/4 - Loading dimensions\")\n",
    "        job._set_run_step(run_id, \"dimensions\")\n",
    "        load_dimensions(run_id, job.src_db, job.dwh_db, job.job_name)\n",
    "        etl_print(\"Dimensions loaded successfully\")\n",
    "\n",
    "        etl_print(\"Step 2/4 - Loading marketing leads\")\n",
    "        job._set_run_step(run_id, \"marketing_leads\")\n",
    "        load_marketing_leads(run_id, job.dwh_db, paths.marketing_xlsx, job)\n",
    "        etl_print(\"Marketing leads loaded\")\n",
    "\n",
    "        etl_print(\"Step 3/4 - Loading sales facts\")\n",
    "        job._set_run_step(run_id, \"sales_facts\")\n",
    "        load_sales_facts(run_id, job.src_db, job.dwh_db, job, run_type)\n",
    "        etl_print(\"Sales facts loaded\")\n",
    "\n",
    "        etl_print(\"Step 4/4 - Loading weblog events\")\n",
    "        job._set_run_step(run_id, \"weblogs\")\n",
    "        load_weblogs(run_id, job.src_db, job.dwh_db, paths.weblog_file, job, run_type)\n",
    "        etl_print(\"Weblog events loaded\")\n",
    "\n",
    "        job._finish_run(run_id, \"succeeded\")\n",
    "        etl_print(\"ETL job completed successfully\")\n",
    "\n",
    "        return run_id\n",
    "\n",
    "    except Exception as e:\n",
    "        etl_print(f\"ETL job failed: {e}\")\n",
    "        job._finish_run(run_id, \"failed\", error_message=str(e))\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be073f1a-f91b-4645-b074-5ec128a382be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETL job initialized\n"
     ]
    }
   ],
   "source": [
    "job = ETLJob(\n",
    "    job_name=\"b2b_etl\",\n",
    "    src_db=paths.src_db,\n",
    "    dwh_db=paths.dwh_db\n",
    ")\n",
    "\n",
    "print(\"ETL job initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c44b152-7006-4305-9093-10e19bca818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-07 06:59:46] Starting ETL job 'b2b_etl' | run_type=initial\n",
      "[2026-02-07 06:59:46] Run ID: f7edcb33-9cbb-4d6b-bcf8-ebb7ef9d9e99\n",
      "[2026-02-07 06:59:46] Step 1/4 - Loading dimensions\n",
      "[2026-02-07 06:59:51] Dimensions loaded successfully\n",
      "[2026-02-07 06:59:51] Step 2/4 - Loading marketing leads\n",
      "[2026-02-07 06:59:59] Marketing leads loaded\n",
      "[2026-02-07 06:59:59] Step 3/4 - Loading sales facts\n",
      "[2026-02-07 07:00:00] Sales facts initial load: processing all orders\n",
      "[2026-02-07 07:00:00] Reading orders + items from source DB\n",
      "[2026-02-07 07:00:02] Inserted 20000 sales rows so far | last_order_id=9889 | skipped_bad=57\n",
      "[2026-02-07 07:00:04] Inserted 40000 sales rows so far | last_order_id=19880 | skipped_bad=117\n",
      "[2026-02-07 07:00:06] Inserted 60000 sales rows so far | last_order_id=29821 | skipped_bad=176\n",
      "[2026-02-07 07:00:08] Inserted 80000 sales rows so far | last_order_id=39725 | skipped_bad=233\n",
      "[2026-02-07 07:00:10] Inserted 100000 sales rows so far | last_order_id=49632 | skipped_bad=298\n",
      "[2026-02-07 07:00:12] Inserted 120000 sales rows so far | last_order_id=59530 | skipped_bad=353\n",
      "[2026-02-07 07:00:14] Inserted 140000 sales rows so far | last_order_id=69494 | skipped_bad=427\n",
      "[2026-02-07 07:00:16] Inserted 160000 sales rows so far | last_order_id=79514 | skipped_bad=492\n",
      "[2026-02-07 07:00:18] Inserted 180000 sales rows so far | last_order_id=89381 | skipped_bad=563\n",
      "[2026-02-07 07:00:21] Inserted 200000 sales rows so far | last_order_id=99358 | skipped_bad=638\n",
      "[2026-02-07 07:00:23] Inserted 220000 sales rows so far | last_order_id=109345 | skipped_bad=699\n",
      "[2026-02-07 07:00:25] Inserted 240000 sales rows so far | last_order_id=119319 | skipped_bad=765\n",
      "[2026-02-07 07:00:27] Inserted 260000 sales rows so far | last_order_id=129296 | skipped_bad=800\n",
      "[2026-02-07 07:00:29] Inserted 280000 sales rows so far | last_order_id=139222 | skipped_bad=857\n",
      "[2026-02-07 07:00:29] Sales facts load complete | inserted_ok=281545 | skipped_bad=860 | last_order_id=140000\n",
      "[2026-02-07 07:00:29] Sales facts loaded\n",
      "[2026-02-07 07:00:29] Step 4/4 - Loading weblog events\n",
      "[2026-02-07 07:00:29] Weblogs load started | file=weblogs_combined.log\n",
      "[2026-02-07 07:00:32] Weblogs progress | processed_ok=25000 | skipped_bad=0 | current_line=25000\n",
      "[2026-02-07 07:00:35] Weblogs progress | processed_ok=50000 | skipped_bad=0 | current_line=50000\n",
      "[2026-02-07 07:00:39] Weblogs progress | processed_ok=75000 | skipped_bad=0 | current_line=75000\n",
      "[2026-02-07 07:00:43] Weblogs progress | processed_ok=100000 | skipped_bad=0 | current_line=100000\n",
      "[2026-02-07 07:00:47] Weblogs progress | processed_ok=125000 | skipped_bad=0 | current_line=125000\n",
      "[2026-02-07 07:00:51] Weblogs progress | processed_ok=150000 | skipped_bad=0 | current_line=150000\n",
      "[2026-02-07 07:00:56] Weblogs progress | processed_ok=175000 | skipped_bad=0 | current_line=175000\n",
      "[2026-02-07 07:01:00] Weblogs progress | processed_ok=200000 | skipped_bad=0 | current_line=200000\n",
      "[2026-02-07 07:01:04] Weblogs progress | processed_ok=225000 | skipped_bad=0 | current_line=225000\n",
      "[2026-02-07 07:01:08] Weblogs progress | processed_ok=250000 | skipped_bad=0 | current_line=250000\n",
      "[2026-02-07 07:01:11] Weblogs load complete | processed_ok=265000 | skipped_bad=0 | last_line=265000\n",
      "[2026-02-07 07:01:11] Weblog events loaded\n",
      "[2026-02-07 07:01:11] ETL job completed successfully\n",
      "Initial ETL completed. run_id: f7edcb33-9cbb-4d6b-bcf8-ebb7ef9d9e99\n"
     ]
    }
   ],
   "source": [
    "run_id = run_etl(job, run_type=\"initial\")\n",
    "print(\"Initial ETL completed. run_id:\", run_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "113582a1-ccca-464e-9b00-0b773c277ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-07 07:01:17] Current checkpoints:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>last_id</th>\n",
       "      <th>last_ts_utc</th>\n",
       "      <th>updated_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>orders</td>\n",
       "      <td>140000</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-02-07T01:30:29.418681+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>weblogs</td>\n",
       "      <td>265000</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-02-07T01:31:11.001777+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_name  last_id last_ts_utc                    updated_at_utc\n",
       "0      orders   140000        None  2026-02-07T01:30:29.418681+00:00\n",
       "1     weblogs   265000        None  2026-02-07T01:31:11.001777+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2026-02-07 07:01:17] Row error counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_name</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>error_type</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2b_db</td>\n",
       "      <td>order_items</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2b_db</td>\n",
       "      <td>end_customers</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing_xlsx</td>\n",
       "      <td>leads</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_name    entity_name        error_type     n\n",
       "0          b2b_db    order_items  validation_error  1720\n",
       "1          b2b_db  end_customers  validation_error    70\n",
       "2  marketing_xlsx          leads  validation_error    60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dwh = connect(paths.dwh_db)\n",
    "\n",
    "cp_df = pd.read_sql_query(\n",
    "    \"SELECT entity_name, last_id, last_ts_utc, updated_at_utc FROM etl_checkpoints WHERE job_name='b2b_etl' ORDER BY entity_name\",\n",
    "    dwh\n",
    ")\n",
    "etl_print(\"Current checkpoints:\")\n",
    "display(cp_df)\n",
    "\n",
    "err_df = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT source_name, entity_name, error_type, COUNT(*) AS n\n",
    "    FROM etl_row_errors\n",
    "    GROUP BY 1,2,3\n",
    "    ORDER BY n DESC\n",
    "    \"\"\",\n",
    "    dwh\n",
    ")\n",
    "etl_print(\"Row error counts:\")\n",
    "display(err_df)\n",
    "\n",
    "dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19c9017d-7e16-4a85-8fd5-d0456b2fe602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report 1 - Top devices\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_family</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desktop</td>\n",
       "      <td>77293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile</td>\n",
       "      <td>51803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bot</td>\n",
       "      <td>25802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tablet</td>\n",
       "      <td>25654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  device_family  events\n",
       "0       desktop   77293\n",
       "1        mobile   51803\n",
       "2           bot   25802\n",
       "3        tablet   25654"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country with most user logins: AR\n",
      "Report 2 - Popular products in the top-login country\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>total_units</th>\n",
       "      <th>total_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PAC-670027</td>\n",
       "      <td>Eco Pack X-36</td>\n",
       "      <td>packaging</td>\n",
       "      <td>1616</td>\n",
       "      <td>727148.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAF-858711</td>\n",
       "      <td>Pro Pack B-59</td>\n",
       "      <td>safety</td>\n",
       "      <td>1596</td>\n",
       "      <td>582838.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SAF-302897</td>\n",
       "      <td>Eco Unit X-43</td>\n",
       "      <td>safety</td>\n",
       "      <td>1526</td>\n",
       "      <td>578947.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOO-256466</td>\n",
       "      <td>Eco Unit B-97</td>\n",
       "      <td>food-service</td>\n",
       "      <td>1519</td>\n",
       "      <td>813406.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAC-226124</td>\n",
       "      <td>Prime Bundle B-28</td>\n",
       "      <td>packaging</td>\n",
       "      <td>1499</td>\n",
       "      <td>702513.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TOO-713099</td>\n",
       "      <td>Pro Unit B-2</td>\n",
       "      <td>tools</td>\n",
       "      <td>1498</td>\n",
       "      <td>558803.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FOO-869295</td>\n",
       "      <td>Ultra Kit Z-47</td>\n",
       "      <td>food-service</td>\n",
       "      <td>1488</td>\n",
       "      <td>665825.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CLE-692960</td>\n",
       "      <td>Ultra Bundle B-23</td>\n",
       "      <td>cleaning</td>\n",
       "      <td>1485</td>\n",
       "      <td>799509.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OFF-593602</td>\n",
       "      <td>Pro Kit X-7</td>\n",
       "      <td>office</td>\n",
       "      <td>1480</td>\n",
       "      <td>620958.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FOO-147936</td>\n",
       "      <td>Max Bundle B-3</td>\n",
       "      <td>food-service</td>\n",
       "      <td>1477</td>\n",
       "      <td>566804.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OFF-318877</td>\n",
       "      <td>Prime Kit X-82</td>\n",
       "      <td>office</td>\n",
       "      <td>1477</td>\n",
       "      <td>692834.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SAF-404771</td>\n",
       "      <td>Pro Pack A-26</td>\n",
       "      <td>safety</td>\n",
       "      <td>1473</td>\n",
       "      <td>698238.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LAB-327797</td>\n",
       "      <td>Pro Kit B-97</td>\n",
       "      <td>lab</td>\n",
       "      <td>1464</td>\n",
       "      <td>842889.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FOO-209159</td>\n",
       "      <td>Ultra Bundle Z-52</td>\n",
       "      <td>food-service</td>\n",
       "      <td>1461</td>\n",
       "      <td>494982.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PAC-876909</td>\n",
       "      <td>Eco Case B-88</td>\n",
       "      <td>packaging</td>\n",
       "      <td>1460</td>\n",
       "      <td>649594.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sku               name      category  total_units  total_value\n",
       "0   PAC-670027      Eco Pack X-36     packaging         1616    727148.45\n",
       "1   SAF-858711      Pro Pack B-59        safety         1596    582838.37\n",
       "2   SAF-302897      Eco Unit X-43        safety         1526    578947.28\n",
       "3   FOO-256466      Eco Unit B-97  food-service         1519    813406.58\n",
       "4   PAC-226124  Prime Bundle B-28     packaging         1499    702513.05\n",
       "5   TOO-713099       Pro Unit B-2         tools         1498    558803.11\n",
       "6   FOO-869295     Ultra Kit Z-47  food-service         1488    665825.35\n",
       "7   CLE-692960  Ultra Bundle B-23      cleaning         1485    799509.11\n",
       "8   OFF-593602        Pro Kit X-7        office         1480    620958.72\n",
       "9   FOO-147936     Max Bundle B-3  food-service         1477    566804.51\n",
       "10  OFF-318877     Prime Kit X-82        office         1477    692834.64\n",
       "11  SAF-404771      Pro Pack A-26        safety         1473    698238.21\n",
       "12  LAB-327797       Pro Kit B-97           lab         1464    842889.02\n",
       "13  FOO-209159  Ultra Bundle Z-52  food-service         1461    494982.11\n",
       "14  PAC-876909      Eco Case B-88     packaging         1460    649594.33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report 3 - Monthly sales (last 12 months)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_month</th>\n",
       "      <th>orders</th>\n",
       "      <th>units</th>\n",
       "      <th>gross_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-02</td>\n",
       "      <td>7512</td>\n",
       "      <td>97843</td>\n",
       "      <td>44108036.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03</td>\n",
       "      <td>8220</td>\n",
       "      <td>106556</td>\n",
       "      <td>47509129.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04</td>\n",
       "      <td>8096</td>\n",
       "      <td>106049</td>\n",
       "      <td>47462581.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05</td>\n",
       "      <td>8486</td>\n",
       "      <td>111804</td>\n",
       "      <td>49759751.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-06</td>\n",
       "      <td>8103</td>\n",
       "      <td>105807</td>\n",
       "      <td>47239160.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-07</td>\n",
       "      <td>8351</td>\n",
       "      <td>109636</td>\n",
       "      <td>49023272.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-08</td>\n",
       "      <td>8198</td>\n",
       "      <td>108032</td>\n",
       "      <td>47823754.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-09</td>\n",
       "      <td>8169</td>\n",
       "      <td>107364</td>\n",
       "      <td>48035123.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-10</td>\n",
       "      <td>8215</td>\n",
       "      <td>108083</td>\n",
       "      <td>48479139.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11</td>\n",
       "      <td>8201</td>\n",
       "      <td>106383</td>\n",
       "      <td>47275836.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2025-12</td>\n",
       "      <td>8345</td>\n",
       "      <td>109215</td>\n",
       "      <td>48706241.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-01</td>\n",
       "      <td>8261</td>\n",
       "      <td>107772</td>\n",
       "      <td>47897131.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-02</td>\n",
       "      <td>1587</td>\n",
       "      <td>20674</td>\n",
       "      <td>9327440.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_month  orders   units  gross_sales\n",
       "0     2025-02    7512   97843  44108036.85\n",
       "1     2025-03    8220  106556  47509129.05\n",
       "2     2025-04    8096  106049  47462581.27\n",
       "3     2025-05    8486  111804  49759751.46\n",
       "4     2025-06    8103  105807  47239160.31\n",
       "5     2025-07    8351  109636  49023272.39\n",
       "6     2025-08    8198  108032  47823754.17\n",
       "7     2025-09    8169  107364  48035123.81\n",
       "8     2025-10    8215  108083  48479139.11\n",
       "9     2025-11    8201  106383  47275836.74\n",
       "10    2025-12    8345  109215  48706241.17\n",
       "11    2026-01    8261  107772  47897131.75\n",
       "12    2026-02    1587   20674   9327440.41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dwh = connect(paths.dwh_db)\n",
    "\n",
    "# Report 1: Top 5 devices used by B2B clients :contentReference[oaicite:4]{index=4}\n",
    "report_1 = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT dd.device_family, COUNT(*) AS events\n",
    "    FROM fact_web_events f\n",
    "    JOIN dim_device dd ON dd.device_key = f.device_key\n",
    "    WHERE f.username IS NOT NULL\n",
    "    GROUP BY dd.device_family\n",
    "    ORDER BY events DESC\n",
    "    LIMIT 5\n",
    "    \"\"\",\n",
    "    dwh\n",
    ")\n",
    "print(\"Report 1 - Top devices\")\n",
    "display(report_1)\n",
    "\n",
    "# Country with most user logins (by events with username)\n",
    "top_country = dwh.execute(\n",
    "    \"\"\"\n",
    "    SELECT country_code\n",
    "    FROM fact_web_events\n",
    "    WHERE username IS NOT NULL AND country_code IS NOT NULL\n",
    "    GROUP BY country_code\n",
    "    ORDER BY COUNT(*) DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ").fetchone()\n",
    "top_country = top_country[0] if top_country else None\n",
    "print(\"Country with most user logins:\", top_country)\n",
    "\n",
    "# Report 2: Most popular products in that country :contentReference[oaicite:5]{index=5}\n",
    "# Interpretation: products most purchased by buyer companies in that country (ties back to B2B sales facts).\n",
    "report_2 = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        dp.sku,\n",
    "        dp.name,\n",
    "        dp.category,\n",
    "        SUM(fs.qty) AS total_units,\n",
    "        ROUND(SUM(fs.line_total), 2) AS total_value\n",
    "    FROM fact_sales fs\n",
    "    JOIN dim_product dp ON dp.product_key = fs.product_key\n",
    "    JOIN dim_company bc ON bc.company_key = fs.buyer_company_key\n",
    "    WHERE bc.country_code = ?\n",
    "    GROUP BY dp.sku, dp.name, dp.category\n",
    "    ORDER BY total_units DESC\n",
    "    LIMIT 15\n",
    "    \"\"\",\n",
    "    dwh,\n",
    "    params=(top_country,)\n",
    ")\n",
    "print(\"Report 2 - Popular products in the top-login country\")\n",
    "display(report_2)\n",
    "\n",
    "# Report 3: Monthly sales for last year :contentReference[oaicite:6]{index=6}\n",
    "# We'll anchor \"last year\" to the max order date in the warehouse, so it stays stable for a demo.\n",
    "max_order_ts = dwh.execute(\"SELECT MAX(order_ts_utc) FROM fact_sales\").fetchone()[0]\n",
    "max_dt = iso_to_dt(max_order_ts) if max_order_ts else utc_now()\n",
    "start_dt = (max_dt.replace(day=1, hour=0, minute=0, second=0, microsecond=0) - timedelta(days=365)).date().isoformat()\n",
    "\n",
    "report_3 = pd.read_sql_query(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        substr(dd.month_start_iso, 1, 7) AS year_month,\n",
    "        COUNT(DISTINCT fs.order_id) AS orders,\n",
    "        SUM(fs.qty) AS units,\n",
    "        ROUND(SUM(fs.line_total), 2) AS gross_sales\n",
    "    FROM fact_sales fs\n",
    "    JOIN dim_date dd ON dd.date_key = fs.date_key\n",
    "    WHERE dd.date_iso >= ?\n",
    "    GROUP BY substr(dd.month_start_iso, 1, 7)\n",
    "    ORDER BY year_month\n",
    "    \"\"\",\n",
    "    dwh,\n",
    "    params=(start_dt,)\n",
    ")\n",
    "print(\"Report 3 - Monthly sales (last 12 months)\")\n",
    "display(report_3)\n",
    "\n",
    "dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5ae0f77-1355-4a6e-8c1d-0a415d5040e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted incremental orders/items into source: 8000\n",
      "[2026-02-07 07:01:57] Starting ETL job 'b2b_etl' | run_type=incremental\n",
      "[2026-02-07 07:01:57] Run ID: 1979bdff-7714-4ded-aca0-f44a40fe6113\n",
      "[2026-02-07 07:01:57] Step 1/4 - Loading dimensions\n",
      "[2026-02-07 07:02:03] Dimensions loaded successfully\n",
      "[2026-02-07 07:02:03] Step 2/4 - Loading marketing leads\n",
      "[2026-02-07 07:02:11] Marketing leads loaded\n",
      "[2026-02-07 07:02:11] Step 3/4 - Loading sales facts\n",
      "[2026-02-07 07:02:11] Sales facts incremental load: starting after order_id=140000\n",
      "[2026-02-07 07:02:11] Reading orders + items from source DB\n",
      "[2026-02-07 07:02:15] Inserted 20000 sales rows so far | last_order_id=146651 | skipped_bad=0\n",
      "[2026-02-07 07:02:16] Sales facts load complete | inserted_ok=24094 | skipped_bad=0 | last_order_id=148000\n",
      "[2026-02-07 07:02:16] Sales facts loaded\n",
      "[2026-02-07 07:02:16] Step 4/4 - Loading weblog events\n",
      "[2026-02-07 07:02:16] Weblogs load started | file=weblogs_combined.log\n",
      "[2026-02-07 07:02:16] Weblogs incremental resume | last_line=265000\n",
      "[2026-02-07 07:02:16] Weblogs load complete | processed_ok=0 | skipped_bad=0 | last_line=265000\n",
      "[2026-02-07 07:02:16] Weblog events loaded\n",
      "[2026-02-07 07:02:16] ETL job completed successfully\n",
      "Incremental ETL completed. run_id: 1979bdff-7714-4ded-aca0-f44a40fe6113\n"
     ]
    }
   ],
   "source": [
    "def simulate_incremental_sales(src_db: str, n_new_orders: int) -> None:\n",
    "    src = connect(src_db)\n",
    "\n",
    "    # get last IDs\n",
    "    last_order_id = src.execute(\"SELECT MAX(order_id) FROM orders\").fetchone()[0] or 0\n",
    "    last_item_id = src.execute(\"SELECT MAX(order_item_id) FROM order_items\").fetchone()[0] or 0\n",
    "\n",
    "    companies = pd.read_sql_query(\"SELECT company_id FROM companies\", src)\n",
    "    suppliers = pd.read_sql_query(\"SELECT supplier_id, company_id FROM suppliers\", src)\n",
    "    customers = pd.read_sql_query(\"SELECT end_customer_id FROM end_customers\", src)\n",
    "    catalog = pd.read_sql_query(\"SELECT buyer_company_id, product_id, supplier_id, catalog_price FROM company_catalog WHERE active_flag=1\", src)\n",
    "\n",
    "    supplier_company_ids = set(suppliers[\"company_id\"].tolist())\n",
    "    buyer_company_ids = [cid for cid in companies[\"company_id\"].tolist() if cid not in supplier_company_ids]\n",
    "\n",
    "    cat_map: Dict[int, List[Tuple[int,int,float]]] = {}\n",
    "    for buyer_id, grp in catalog.groupby(\"buyer_company_id\"):\n",
    "        cat_map[int(buyer_id)] = [(int(r.product_id), int(r.supplier_id), float(r.catalog_price)) for r in grp.itertuples(index=False)]\n",
    "\n",
    "    now = utc_now()\n",
    "    statuses = [\"created\", \"confirmed\", \"shipped\", \"delivered\", \"cancelled\"]\n",
    "    status_w = [0.06, 0.22, 0.18, 0.50, 0.04]\n",
    "\n",
    "    new_orders = []\n",
    "    new_items = []\n",
    "    order_item_id = last_item_id + 1\n",
    "\n",
    "    for i in range(1, n_new_orders + 1):\n",
    "        order_id = last_order_id + i\n",
    "        buyer = random.choice([b for b in buyer_company_ids if b in cat_map])\n",
    "        cust = int(random.choice(customers[\"end_customer_id\"].tolist()))\n",
    "        order_ts = now - timedelta(days=random.randint(0, 25), seconds=random.randint(0, 86400))\n",
    "        created = order_ts - timedelta(minutes=random.randint(1, 60))\n",
    "        status = weighted_choice(statuses, status_w)\n",
    "        currency = \"USD\" if random.random() < 0.18 else \"ARS\"\n",
    "\n",
    "        new_orders.append((order_id, buyer, cust, dt_to_iso(order_ts), status, currency, dt_to_iso(created)))\n",
    "\n",
    "        k = random.randint(1, 5)\n",
    "        picks = random.sample(cat_map[buyer], k=k)\n",
    "        for (pid, sid, unit_price) in picks:\n",
    "            qty = random.randint(1, 10)\n",
    "            line_total = round(unit_price * qty, 2)\n",
    "            new_items.append((order_item_id, order_id, pid, sid, qty, unit_price, line_total, dt_to_iso(order_ts)))\n",
    "            order_item_id += 1\n",
    "\n",
    "    src.execute(\"BEGIN;\")\n",
    "    src.executemany(\n",
    "        \"INSERT INTO orders(order_id, buyer_company_id, end_customer_id, order_ts_utc, status, currency, created_at_utc) VALUES (?,?,?,?,?,?,?)\",\n",
    "        new_orders\n",
    "    )\n",
    "    src.executemany(\n",
    "        \"INSERT INTO order_items(order_item_id, order_id, product_id, supplier_id, qty, unit_price, line_total, created_at_utc) VALUES (?,?,?,?,?,?,?,?)\",\n",
    "        new_items\n",
    "    )\n",
    "    src.commit()\n",
    "    src.close()\n",
    "\n",
    "\n",
    "simulate_incremental_sales(paths.src_db, INCR_NEW_ORDERS)\n",
    "print(\"Inserted incremental orders/items into source:\", INCR_NEW_ORDERS)\n",
    "\n",
    "# Run incremental ETL\n",
    "run_id_incr = run_etl(job, run_type=\"incremental\")\n",
    "print(\"Incremental ETL completed. run_id:\", run_id_incr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16553ac5-2d16-409b-bf0d-e027f7623d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest ETL runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_type</th>\n",
       "      <th>status</th>\n",
       "      <th>started_at_utc</th>\n",
       "      <th>finished_at_utc</th>\n",
       "      <th>last_step</th>\n",
       "      <th>error_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1979bdff-7714-4ded-aca0-f44a40fe6113</td>\n",
       "      <td>incremental</td>\n",
       "      <td>succeeded</td>\n",
       "      <td>2026-02-07T01:31:57.590393+00:00</td>\n",
       "      <td>2026-02-07T01:32:16.653201+00:00</td>\n",
       "      <td>weblogs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f7edcb33-9cbb-4d6b-bcf8-ebb7ef9d9e99</td>\n",
       "      <td>initial</td>\n",
       "      <td>succeeded</td>\n",
       "      <td>2026-02-07T01:29:46.336726+00:00</td>\n",
       "      <td>2026-02-07T01:31:11.016275+00:00</td>\n",
       "      <td>weblogs</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8d2dc456-072c-4267-bbf2-21f3fa01cae7</td>\n",
       "      <td>initial</td>\n",
       "      <td>failed</td>\n",
       "      <td>2026-02-07T01:26:46.098501+00:00</td>\n",
       "      <td>2026-02-07T01:28:02.171624+00:00</td>\n",
       "      <td>weblogs</td>\n",
       "      <td>name 'LOG_RE' is not defined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 run_id     run_type     status  \\\n",
       "0  1979bdff-7714-4ded-aca0-f44a40fe6113  incremental  succeeded   \n",
       "1  f7edcb33-9cbb-4d6b-bcf8-ebb7ef9d9e99      initial  succeeded   \n",
       "2  8d2dc456-072c-4267-bbf2-21f3fa01cae7      initial     failed   \n",
       "\n",
       "                     started_at_utc                   finished_at_utc  \\\n",
       "0  2026-02-07T01:31:57.590393+00:00  2026-02-07T01:32:16.653201+00:00   \n",
       "1  2026-02-07T01:29:46.336726+00:00  2026-02-07T01:31:11.016275+00:00   \n",
       "2  2026-02-07T01:26:46.098501+00:00  2026-02-07T01:28:02.171624+00:00   \n",
       "\n",
       "  last_step                 error_message  \n",
       "0   weblogs                          None  \n",
       "1   weblogs                          None  \n",
       "2   weblogs  name 'LOG_RE' is not defined  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_name</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>last_id</th>\n",
       "      <th>last_ts_utc</th>\n",
       "      <th>updated_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2b_etl</td>\n",
       "      <td>orders</td>\n",
       "      <td>148000</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-02-07T01:32:16.248119+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2b_etl</td>\n",
       "      <td>weblogs</td>\n",
       "      <td>265000</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-02-07T01:32:16.624120+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  job_name entity_name  last_id last_ts_utc                    updated_at_utc\n",
       "0  b2b_etl      orders   148000        None  2026-02-07T01:32:16.248119+00:00\n",
       "1  b2b_etl     weblogs   265000        None  2026-02-07T01:32:16.624120+00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row errors summary (expected: some validation errors we injected)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_name</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>error_type</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b2b_db</td>\n",
       "      <td>order_items</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b2b_db</td>\n",
       "      <td>end_customers</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing_xlsx</td>\n",
       "      <td>leads</td>\n",
       "      <td>validation_error</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source_name    entity_name        error_type     n\n",
       "0          b2b_db    order_items  validation_error  1720\n",
       "1          b2b_db  end_customers  validation_error   105\n",
       "2  marketing_xlsx          leads  validation_error    90"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dwh = connect(paths.dwh_db)\n",
    "\n",
    "runs = pd.read_sql_query(\n",
    "    \"SELECT run_id, run_type, status, started_at_utc, finished_at_utc, last_step, error_message FROM etl_runs ORDER BY started_at_utc DESC LIMIT 10\",\n",
    "    dwh\n",
    ")\n",
    "print(\"Latest ETL runs\")\n",
    "display(runs)\n",
    "\n",
    "cps = pd.read_sql_query(\n",
    "    \"SELECT job_name, entity_name, last_id, last_ts_utc, updated_at_utc FROM etl_checkpoints ORDER BY entity_name\",\n",
    "    dwh\n",
    ")\n",
    "print(\"Checkpoints\")\n",
    "display(cps)\n",
    "\n",
    "errs = pd.read_sql_query(\n",
    "    \"SELECT source_name, entity_name, error_type, COUNT(*) AS n FROM etl_row_errors GROUP BY 1,2,3 ORDER BY n DESC\",\n",
    "    dwh\n",
    ")\n",
    "print(\"Row errors summary (expected: some validation errors we injected)\")\n",
    "display(errs)\n",
    "\n",
    "dwh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4a22c-5ccf-4910-9c71-ce60b249f8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41b35e-203a-4ee4-8bf7-361879a6b4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
